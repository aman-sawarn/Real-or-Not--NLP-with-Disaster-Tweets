{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning BERT with Keras and tf.Module.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RgldLkFrrL1",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning BERT with Keras and tf.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW8wMuz1rRZU",
        "colab_type": "text"
      },
      "source": [
        "In this experiment we convert a pre-trained BERT model checkpoint into a trainable Keras layer, which we use to solve a sentence pair classification task.\n",
        "\n",
        "We achieve this using a tf.Module, which is a neat abstraction designed to handle pre-trained Tensorflow models.\n",
        "Exported modules can be easily integrated into other models, which facilitates experiments with powerful NN architectures.\n",
        "\n",
        "The plan for this experiment is:\n",
        "\n",
        "1.   getting a pre-trained BERT model checkpoint\n",
        "2.   defining the specification of the tf.Module\n",
        "3.   exporting the module\n",
        "4.   building the text preprocessing pipeline\n",
        "5.   implementing a custom Keras layer\n",
        "6.   training a Keras model to solve a sentence-pair classification task\n",
        "\n",
        "\n",
        "# What is in this guide?\n",
        "This guide is about integrating pre-trained Tensorflow models into Keras pipelines. It contains implementations of two things: a BERT tf.Module and a Keras layer built on top of it.\n",
        "# What does it take?\n",
        "For a reader familiar with TensorFlow it should take around 30 minutes to finish this guide.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "outputId": "caba9ab9-c7f5-41dd-ea4d-c6a54a7b76bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import auth, drive\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert_repo'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Total 336 (delta 0), reused 0 (delta 0), pack-reused 336\u001b[K\n",
            "Receiving objects: 100% (336/336), 283.40 KiB | 951.00 KiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNM1WYKCpxtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veuCjAVtRsuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "847a8a2e-2963-40ee-b9e1-92e420b905d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-30 17:48:29--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 2a00:1450:400c:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  81.6MB/s    in 4.8s    \n",
            "\n",
            "2019-11-30 17:48:34 (81.6 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0iVw5dkr9A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: building a tf.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLQ8xyKLsEn5",
        "colab_type": "text"
      },
      "source": [
        "**tf.Modules** are designed to provide a simple way to manipulate reusable parts of pre-trained machine learning models in Tensorflow. Google maintains a curated library of such modules at tf.Hub. In this guide however, we will build one by ourselves.\n",
        "\n",
        "To that end, we will need to implement a ***module_fn*** which will contain the full specification of the module inner workings. \n",
        "We begin by defining input placeholders. Then the BERT graph is created from a configuration file passed through ***config_path***. Then we model outputs are defined: the final encoder layer to seq_output and pooled *'**CLS**'* token representation to pool_output.\n",
        "\n",
        "Additionally, extra assets may be bundled with the module. In this example, we add a ***vocab_file*** containing the WordPiece vocabulary to the module assets. As a result, the vocabulary file will be exported with the module, which will make it self-contained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "        \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5dJoFIGsfYI",
        "colab_type": "text"
      },
      "source": [
        "Finally, we define signatures, which are particular transformations of inputs to outputs exposed to module consumers. One could think of it as a module interface with the outside world.\n",
        "\n",
        "Here we add two signatures to the module: one that takes raw text features as input and returns computed text representations as output. The other takes no inputs and returns the path to vocabulary file and lowercase flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocsQWnDiRqcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: exporting the module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muib1AD4ssN2",
        "colab_type": "text"
      },
      "source": [
        "Now that the module_fn is defined, we can use it to build and export the module. Passing the tags_and_args argument to create_module_spec will result in two graph variants being added to the module: for training with tags ***{\"train\"}*** and for inference with an empty set of tags. This allows to control dropout, which is disabled at inference time, and enabled during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "outputId": "f95175e4-c97b-42c8-8459-1d8fdb1e0c05",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "MODEL_DIR = \"uncased_L-12_H-768_A-12\" #@param {type:\"string\"} ['uncased_L-12_H-768_A-12']\n",
        "\n",
        "config_path = \"/content/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/{}/bert_model.ckpt\".format(MODEL_DIR))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "From bert_repo/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "From bert_repo/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "From bert_repo/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "From bert_repo/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "From bert_repo/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py:110: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQeLyV6vYMlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWWY-PeRs9p1",
        "colab_type": "text"
      },
      "source": [
        "The BERT model requires that text is represented as 3 matrices containing ***input_ids***, ***input_mask***, and ***segment_ids***. In this step we build a pipeline which takes a list of strings, and outputs these three matrices, as simple as that.\n",
        "\n",
        "First of all, raw input text is converted into ***InputExamples***. If the input text is a sentence pair, separated by a special '|||' sequence, the sentences are split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la7nsfojtI-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        text_a = None\n",
        "        text_b = None\n",
        "        m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
        "        if m is None:\n",
        "            text_a = line\n",
        "        else:\n",
        "            text_a = m.group(1)\n",
        "            text_b = m.group(2)\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCby58ctJFQ",
        "colab_type": "text"
      },
      "source": [
        "***InputExamples*** are then tokenized and converted to ***InputFeatures*** using the ***convert_examples_to_features*** function from the original repository. However, we will require these features to be converted to np.arrays to use with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNE1vP_ktgbn",
        "colab_type": "text"
      },
      "source": [
        "Finally, let us put it all together in a single pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "  tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "  \n",
        "  def strings_to_arrays(sents):\n",
        "  \n",
        "      sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "      examples = []\n",
        "      for example in read_examples(sents):\n",
        "          examples.append(example)\n",
        "\n",
        "      features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "      arrays = features_to_arrays(features)\n",
        "      return arrays\n",
        "  \n",
        "  return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HDop0a4tixD",
        "colab_type": "text"
      },
      "source": [
        "All done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: implementing a BERT Keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS8FTjkh_maW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: sentence pair classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJF9vqwgtycK",
        "colab_type": "text"
      },
      "source": [
        "Now let us try the layer on a real-world dataset. For this part we will use the [Quora Question Pairs](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs) dataset which consists of over 400,000 potential question duplicate pairs labeled for semantic equivalence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxRGaEW1g70T",
        "colab_type": "code",
        "outputId": "34222d2c-b00d-4ecb-84df-70ec3b72e478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv -O quora_train.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-30 17:49:26--  http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\n",
            "Resolving qim.fs.quoracdn.net (qim.fs.quoracdn.net)... 151.101.1.2, 151.101.65.2, 151.101.129.2, ...\n",
            "Connecting to qim.fs.quoracdn.net (qim.fs.quoracdn.net)|151.101.1.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58176133 (55M) [text/tab-separated-values]\n",
            "Saving to: ‘quora_train.tsv’\n",
            "\n",
            "quora_train.tsv     100%[===================>]  55.48M   306MB/s    in 0.2s    \n",
            "\n",
            "2019-11-30 17:49:28 (306 MB/s) - ‘quora_train.tsv’ saved [58176133/58176133]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAbLo0y7g7xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6BanaWyuKQ-",
        "colab_type": "text"
      },
      "source": [
        "We join the question pairs with the \"|||\" sequence and split them into train and test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtqfzXJYADug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"quora_train.tsv\", sep='\\t')\n",
        "\n",
        "labels = df.is_duplicate.values\n",
        "\n",
        "texts = []\n",
        "delimiter = \" ||| \"\n",
        "for q1, q2 in zip(df.question1.tolist(), df.question2.tolist()):\n",
        "  texts.append(delimiter.join((str(q1), str(q2))))\n",
        "\n",
        "texts = np.array(texts)\n",
        "\n",
        "trX, tsX, trY, tsY = train_test_split(texts, labels, shuffle=True, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFuVy0K6bnE0",
        "colab_type": "text"
      },
      "source": [
        "Building and training a sentence-pair classification model is straighforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "d57b7257-8759-45e0-932d-2b3e34f54c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=48, tune_embeddings=False,\n",
        "                    pooling='cls', n_tune_layers=3, verbose=False)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(encoder(inp))\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "outputId": "5b735922-dae2-4f21-f399-cad73b88afd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ),\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer (BertLayer)       (None, 768)               109482240 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 109,483,009\n",
            "Trainable params: 21,854,977\n",
            "Non-trainable params: 87,628,032\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6svZ93niek0",
        "colab_type": "code",
        "outputId": "b2e86718-ad95-4f20-f297-8defeb64b3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_tuned.hdf5\")\n",
        "\n",
        "model.fit(trX, trY, validation_data=[tsX, tsY], batch_size=128, epochs=5, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 323432 samples, validate on 80858 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/extract_features.py:283: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "323432/323432 [==============================] - 3197s 10ms/sample - loss: 0.3659 - acc: 0.8255 - val_loss: 0.3198 - val_acc: 0.8551\n",
            "Epoch 2/5\n",
            "323432/323432 [==============================] - 3191s 10ms/sample - loss: 0.2898 - acc: 0.8704 - val_loss: 0.2896 - val_acc: 0.8723\n",
            "Epoch 3/5\n",
            "323432/323432 [==============================] - 3231s 10ms/sample - loss: 0.2480 - acc: 0.8920 - val_loss: 0.2833 - val_acc: 0.8765\n",
            "Epoch 4/5\n",
            "323432/323432 [==============================] - 3205s 10ms/sample - loss: 0.2083 - acc: 0.9123 - val_loss: 0.2839 - val_acc: 0.8814\n",
            "Epoch 5/5\n",
            "323432/323432 [==============================] - 3244s 10ms/sample - loss: 0.1671 - acc: 0.9325 - val_loss: 0.2957 - val_acc: 0.8831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6a543ab198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAPBAcVBbxTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6oHAFhcieiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cJ58dwPiefI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meD5_jLX8HWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "outputId": "8bc5a6e4-d5b6-4880-f047-fcde1acf6d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.predict(trX[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.66316462e-01],\n",
              "       [1.98765397e-02],\n",
              "       [9.18453693e-01],\n",
              "       [2.75392830e-02],\n",
              "       [1.78414583e-03],\n",
              "       [9.21200871e-01],\n",
              "       [4.85856265e-01],\n",
              "       [7.82228947e-01],\n",
              "       [8.56636558e-04],\n",
              "       [1.22578345e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"model.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "outputId": "78ca754e-a920-40f8-a852-007e47edd80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"model.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_tuned.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "outputId": "1677d161-7e91-4cc0-cfa5-b38555957b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.predict(trX[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.66316462e-01],\n",
              "       [1.98765397e-02],\n",
              "       [9.18453693e-01],\n",
              "       [2.75392830e-02],\n",
              "       [1.78414583e-03],\n",
              "       [9.21200871e-01],\n",
              "       [4.85856265e-01],\n",
              "       [7.82228947e-01],\n",
              "       [8.56636558e-04],\n",
              "       [1.22578345e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlXawIFPofGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnJa8oWJ2u8H",
        "colab_type": "text"
      },
      "source": [
        "In some cases (e.g. when serving), one might want to optimize the trained model for maximum inference throughput. In TensorFlow this can be achieved by \"freezing\" the model. \n",
        "\n",
        "During \"freezing\" the model variables are replaced by constants, and the nodes required for training are pruned from the computational graph. The resulting graph becomes more lightweight, requires less RAM and achieves better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPpYP_ge55ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjbufyTp5-90",
        "colab_type": "text"
      },
      "source": [
        "We freeze our trained model and write the serialized graph to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "outputId": "e5c60228-df67-48b6-962a-b3eefe78664d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph.pb\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/strip_unused_lib.py:88: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
            "WARNING:tensorflow:From <ipython-input-17-0f60630b9ef3>:34: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6cljim3u_3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI77AK8w6no7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXePoLYV6n27",
        "colab_type": "text"
      },
      "source": [
        "Now let's restore the frozen graph and do some inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz7gbQp0douz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khWpzTjPvUZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "outputId": "3417e5c9-a701-49f9-ac11-c63fc88314a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph.pb\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/bert_experimental/finetuning/graph_ops.py:39: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/bert_experimental/finetuning/graph_ops.py:40: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZXDHbHd7eCx",
        "colab_type": "text"
      },
      "source": [
        "To run inference we need to get the handles for input and output tensors of the graph. This part a little tricky: we retrieve a list of all operations in the restored graph and then manually get the names of relevant ops. The list is sorted, so in this case it is enough to take the first and the last operation.\n",
        "\n",
        " To get the Tensor name we append **\":0\"** to the op name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "outputId": "f8fb8759-e44a-4d88-a6f4-1033faf7cb33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import/input_1_1 import/dense_1/Sigmoid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDsG0Pkw88f-",
        "colab_type": "text"
      },
      "source": [
        "The preprocessing function we injected into the Keras layer is not serializable and was not restored in the new graph. No worries though - we can simply define it again with the same name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKDhrawB9n1G",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can get the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IbB4pjlCcmY",
        "colab_type": "code",
        "outputId": "793722c7-d007-43d6-a82d-bc071a6e7fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "trX[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"What is the world's view on the design of India's new ₹500 and ₹2000 notes? ||| What do you think about the new design of 500 and 2000 rupee notes?\",\n",
              "       \"Where does the Venus Express go since it's dead now? ||| Where is Venus right now (June 2016)?\",\n",
              "       'How can I learn communication skills? ||| How we improve our communication skills?',\n",
              "       'How is energy stored in gasoline? ||| How is energy stored?',\n",
              "       'How many goals did Messi score in his career? ||| Will Lionel Messi cancel his retirement?',\n",
              "       'What were the major effects of the cambodia earthquake, and how do these effects compare to the Banda Sea earthquake in 1938? ||| What were the major effects of the cambodia earthquake, and how do these effects compare to the Concepcion earthquake in 1751?',\n",
              "       'How would you know if a shy guy likes you? ||| How do I know if a guy I like is shy?',\n",
              "       'What is it like to work for AT&T? ||| What was it like to work at AT&T?',\n",
              "       \"I have a Master's in English literature. Should I repeat it in USA for better jobs there? My GPA is 4.96 on 5.00 ||| What are the benefits of like-minded connections?\",\n",
              "       'What is a typical workday like for a neurosurgeon? ||| How does it feel to be a neurosurgeon?'],\n",
              "      dtype='<U1323')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "outputId": "ea99062c-c66a-49da-ff55-ab9b69775769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: trX[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.66316462e-01],\n",
              "       [1.98765397e-02],\n",
              "       [9.18453693e-01],\n",
              "       [2.75392830e-02],\n",
              "       [1.78414583e-03],\n",
              "       [9.21200871e-01],\n",
              "       [4.85856265e-01],\n",
              "       [7.82228947e-01],\n",
              "       [8.56636558e-04],\n",
              "       [1.22578345e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7GX13v2iyzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Jh5rpZlFx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qxOCE1vlI0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IruxM0xkrBja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}